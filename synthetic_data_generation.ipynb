{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Industrial Data with a Hybrid GAN\n",
    "\n",
    "Welcome! In this notebook, we're going to tackle a common problem in data science: not having enough data. Our solution? We'll teach an AI to create realistic, synthetic data for us.\n",
    "\n",
    "We'll be working with a simulated dataset for predictive maintenance in machines. Our approach is a bit of a hybrid: we'll use a ready-made model called **CTGAN** (which is great for tabular data) and also build our own **WGAN** (Wasserstein GAN) from scratch to see how they compare and combine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Getting Our Tools Ready\n",
    "\n",
    "First things first, let's import all the Python libraries we'll need for this experiment. If you haven't installed them yet, you can use the `requirements.txt` file in this repository."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For building our own GAN with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# For the pre-built CTGAN model\n",
    "from ctgan import CTGAN\n",
    "\n",
    "# For data scaling and visualization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"All libraries are ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Creating Our \"Real\" Dataset\n",
    "\n",
    "Before we can generate fake data, we need some real data to learn from. Since we don't have a real industrial dataset on hand, we'll simulate one. This will be our \"ground truth.\"\n",
    "\n",
    "We'll also scale the numerical features to a range between 0 and 1. This is a common practice that helps GANs train more effectively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's create some predictable randomness for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Building a DataFrame with simulated machine sensor data\n",
    "real_industrial_data = pd.DataFrame({\n",
    "    'temperature': np.random.uniform(20, 100, 1000),\n",
    "    'pressure': np.random.uniform(1, 10, 1000),\n",
    "    'vibration': np.random.uniform(0.1, 5.0, 1000),\n",
    "    'machine_age': np.random.randint(1, 10, 1000),\n",
    "    'failure': np.random.choice([0, 1], size=1000) # 0 = No Failure, 1 = Failure\n",
    "})\n",
    "\n",
    "# GANs work best with normalized data, so let's scale our features to be between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "scaled_real_data = pd.DataFrame(scaler.fit_transform(real_industrial_data), columns=real_industrial_data.columns)\n",
    "\n",
    "print(\"Here's a peek at our scaled 'real' data:\")\n",
    "scaled_real_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the CTGAN\n",
    "\n",
    "Now for our first model. CTGAN is specialized for generating tabular data like ours. We just need to point it to our data, tell it which columns are discrete (like our 'failure' column), and let it train."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize the CTGAN model. We'll train it for 50 epochs.\n",
    "ctgan_model = CTGAN(epochs=50)\n",
    "\n",
    "# Train the model on our scaled data. We need to tell it which columns are categorical/discrete.\n",
    "ctgan_model.fit(scaled_real_data, discrete_columns=['failure'])\n",
    "\n",
    "# Let's generate 500 new samples!\n",
    "ctgan_synthetic_samples = ctgan_model.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Building and Training Our Own WGAN\n",
    "\n",
    "This is the fun part! We're building a WGAN from scratch. A GAN has two main parts:\n",
    "- **The Generator**: The \"artist\" that tries to create realistic-looking data from random noise.\n",
    "- **The Discriminator**: The \"critic\" that tries to tell the difference between real data and the Generator's fake data.\n",
    "\n",
    "They train in a cat-and-mouse game. The Generator gets better at making fakes, and the Discriminator gets better at spotting them. Over time, the Generator becomes a master forger, creating very realistic data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# A helper class to prepare our data for PyTorch\n",
    "class IndustrialDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# The Generator network - our 'artist'\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh() # Tanh ensures the output is between -1 and 1 (we can adjust later if needed)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# The Discriminator network - our 'critic'\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1) # Outputs a single score indicating how 'real' it thinks the data is\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Training Setup ---\n",
    "# Create instances of our models\n",
    "latent_space_dim = 10\n",
    "data_dim = scaled_real_data.shape[1]\n",
    "wgan_generator = Generator(input_dim=latent_space_dim, output_dim=data_dim)\n",
    "wgan_discriminator = Discriminator(input_dim=data_dim)\n",
    "\n",
    "# Set up the optimizers\n",
    "optimizer_G = optim.RMSprop(wgan_generator.parameters(), lr=0.00005)\n",
    "optimizer_D = optim.RMSprop(wgan_discriminator.parameters(), lr=0.00005)\n",
    "\n",
    "# Prepare the data loader\n",
    "real_data_torch = IndustrialDataset(scaled_real_data)\n",
    "loader = DataLoader(real_data_torch, batch_size=64, shuffle=True)\n",
    "\n",
    "# --- The Training Loop ---\n",
    "print(\"Starting WGAN training...\")\n",
    "for epoch in range(50):\n",
    "    for i, real_samples in enumerate(loader):\n",
    "        # --- Train the Critic (Discriminator) ---\n",
    "        # The critic needs to get better at telling real from fake.\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Generate some fake samples\n",
    "        z = torch.randn(real_samples.size(0), latent_space_dim)\n",
    "        fake_samples = wgan_generator(z).detach() # .detach() so we don't train the generator here\n",
    "        \n",
    "        # The WGAN loss aims to maximize the distance between the critic's scores for real vs. fake data\n",
    "        loss_D = -(torch.mean(wgan_discriminator(real_samples)) - torch.mean(wgan_discriminator(fake_samples)))\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # This is a key part of WGAN: clamp the critic's weights to a small range.\n",
    "        # It prevents the critic from getting too confident, which keeps the training stable.\n",
    "        for p in wgan_discriminator.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # --- Train the Artist (Generator) ---\n",
    "        # The artist's goal is to fool the critic.\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate a new batch of fake samples\n",
    "        z = torch.randn(real_samples.size(0), latent_space_dim)\n",
    "        fake_samples = wgan_generator(z)\n",
    "        \n",
    "        # We want to maximize the critic's score for fake samples (i.e., make it think they're real).\n",
    "        loss_G = -torch.mean(wgan_discriminator(fake_samples))\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Discriminator Loss: {loss_D.item():.4f}, Generator Loss: {loss_G.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generating Data with Our WGAN\n",
    "\n",
    "Now that our custom WGAN is trained, let's use the generator to create another 500 synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create some random noise to feed our generator\n",
    "z_noise = torch.randn(500, latent_space_dim)\n",
    "\n",
    "# Generate the scaled synthetic data\n",
    "wgan_synthetic_samples = wgan_generator(z_noise).detach().numpy()\n",
    "wgan_synthetic_samples = pd.DataFrame(wgan_synthetic_samples, columns=real_industrial_data.columns)\n",
    "\n",
    "print(\"WGAN synthetic samples generated.\")\n",
    "wgan_synthetic_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Creating the Hybrid Dataset\n",
    "\n",
    "Time to combine our efforts! We'll take the samples from both the CTGAN and our WGAN and merge them into one final synthetic dataset. We'll also use our scaler to transform the data back from the 0-1 range to its original scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Combine the synthetic data from both models\n",
    "hybrid_synthetic_data_scaled = pd.concat([ctgan_synthetic_samples, wgan_synthetic_samples], ignore_index=True)\n",
    "\n",
    "# Convert the scaled data back to its original range\n",
    "hybrid_synthetic_data = pd.DataFrame(scaler.inverse_transform(hybrid_synthetic_data_scaled), columns=real_industrial_data.columns)\n",
    "\n",
    "# Save our final creation to a CSV file\n",
    "hybrid_synthetic_data.to_csv('synthetic_industrial_data.csv', index=False)\n",
    "\n",
    "print(\"Synthetic Hybrid Dataset Generated and Saved Successfully!\")\n",
    "hybrid_synthetic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: The Moment of Truth - Visualization\n",
    "\n",
    "Did our hybrid GAN do a good job? The best way to find out is to visualize it. We'll plot the distributions of the features from our original data and our new synthetic data side-by-side. If the plots look similar, we've succeeded!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set a nice style for our plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# --- Plotting the Distributions ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Comparing Real vs. Synthetic Data Distributions', fontsize=16)\n",
    "\n",
    "# Temperature\n",
    "sns.histplot(real_industrial_data['temperature'], kde=True, ax=axes[0, 0], color='blue', label='Original')\n",
    "sns.histplot(hybrid_synthetic_data['temperature'], kde=True, ax=axes[0, 0], color='orange', label='Synthetic')\n",
    "axes[0, 0].set_title('Temperature Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Pressure\n",
    "sns.histplot(real_industrial_data['pressure'], kde=True, ax=axes[0, 1], color='blue', label='Original')\n",
    "sns.histplot(hybrid_synthetic_data['pressure'], kde=True, ax=axes[0, 1], color='orange', label='Synthetic')\n",
    "axes[0, 1].set_title('Pressure Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Vibration\n",
    "sns.histplot(real_industrial_data['vibration'], kde=True, ax=axes[1, 0], color='blue', label='Original')\n",
    "sns.histplot(hybrid_synthetic_data['vibration'], kde=True, ax=axes[1, 0], color='orange', label='Synthetic')\n",
    "axes[1, 0].set_title('Vibration Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Machine Age\n",
    "sns.histplot(real_industrial_data['machine_age'], kde=True, ax=axes[1, 1], color='blue', label='Original')\n",
    "sns.histplot(hybrid_synthetic_data['machine_age'], kde=True, ax=axes[1, 1], color='orange', label='Synthetic')\n",
    "axes[1, 1].set_title('Machine Age Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# --- Plotting the Relationship between two variables ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x=real_industrial_data['temperature'], y=real_industrial_data['pressure'], color='blue', label='Original', alpha=0.6)\n",
    "sns.scatterplot(x=hybrid_synthetic_data['temperature'], y=hybrid_synthetic_data['pressure'], color='orange', label='Synthetic', alpha=0.6)\n",
    "ax.set_title('Scatter Plot: Real vs Synthetic (Temperature vs Pressure)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
